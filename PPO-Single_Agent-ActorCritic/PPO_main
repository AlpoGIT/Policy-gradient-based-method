## POLICY GRADIENT -- PPO ##
import itertools
import numpy as np
import torch, torch.nn as nn
import gym
from collections import deque, namedtuple
import matplotlib.pyplot as plt
import time, random
from network import  actor_critic_continuous, storage, actor_critic_continuous_covariant
#device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device = torch.device("cpu")
########################################################
#               Define environment                     #
########################################################
#env_name = 'LunarLander-v2'
#env_name = 'LunarLanderContinuous-v2' #s = 8, a = 2
#env_name = 'CartPole-v0'
#env_name = 'MountainCarContinuous-v0'
#env_name = 'MountainCar-v0' # PPO doesn't work ?

target_score = 30


# you need to install in p2_continuous-control
from unityagents import UnityEnvironment
env = UnityEnvironment(file_name="C:\\Users\AL\Documents\GitHub\deep-reinforcement-learning\p2_continuous-control\Reacher_Windows_x86_64\Reacher.exe", no_graphics=True)

# get the default brain
brain_name = env.brain_names[0]
brain = env.brains[brain_name]

#action_size = brain.vector_action_space_size
















#env = gym.make(env_name)
state_dim = 33 # env.observation_space.shape[0]
action_dim = 4 # env.action_space.shape[0]#.n

'''
print(env_name)
print("state space: \t{}".format(env.observation_space))
print("action space: \t{}".format(env.action_space))
'''

keep_print_every = 20
print_every = 1
time_limit = 10000
gradient_clip = 10

# learn value function first, then actor will learn
c_1 = 1. # the weight of the value/critic constraint to optimize (between 0 and 1)

######################################################################################################
#               hyperparameters optimization
######################################################################################################
'''
without entropy
(mini_batch_size, rollout_length)
(32, above) 1. at 100
(32, 2048)  1.7 at 100
(32, 1536)  1.9 at 100
(32, 1280)  1.9 at 100
(32, 1152)  2.3 at 100      <---------------- optimal
(32, 1024)  2. at 100
(32, 768)   1.2 at 100
(32, 512)   1.7 at 100
(32, 256)   1.2 at 100
'''

rollout_length = 1152 # optimal
mini_batch_size = 32  # optimal


'''
with entropy
beta
0.01    1.7 at 100      <------------------- optimal
0.05    1.36 at 100
0.1     1. at 100
'''

beta = 0.01 


'''
optimization_epoch
2       1.63, 1.26 at 100   <-------------- optimal ? (lots of variance; highly dependend on starting point)
3       1.5, 1.12 at 100  
4       1.23
6       1.65 at 100
9       1.24 at 100
12      0.83 at 100
'''

optimization_epoch = 3  # try   6   9   12  15


'''
alpha
1e-5    0.62 at 100
1e-4    1.62 at 100     <----------- optimal
1e-3    1.
1e-2
1e-1
'''
alpha = 1e-4  # try 0.00001   0.001   0.01    0.1

'''
epsilon (PPO clip)
0.1         1.41 at 100
0.2
0.3         1.62 at 100             <------------ optimal
0.4
0.5         1.34 at 100
'''
eps = 0.2   # try   0.1 0.2 0.4 0.5

'''
optimizer:
    but you need to tune alpha ...
adam        1.62 at 100
SGD
RMSprop
'''

'''
Entropy decay: strong at beginning then decay to zero
beta = beta * entropy_decay
implement
0.992 
'''
#entropy_decay = 0.992
######################################################################################################
'''                 end of hyperparameters optimization

After that find bugs:
                        1- try the PPO code of lunar lander... if comparable results or not???
                        2- try with covariance matrix ?? it should be necessary
                        3- compare your code with lunar lander code again...
                        4- compare your code with Zhang again...
                        5- small details... prevent from convergence (like normalization, etc)
                        6- without entropy

                         - No need to normalize (advantage or rewards) in this case



'''
######################################################################################################




GAE_tau = 0.95
discount = 0.99
buffer_size = 1000000





memory = storage(buffer_size, mini_batch_size)


network = actor_critic_continuous(state_dim, action_dim).to(device)
# slow and not necessary
#network = actor_critic_continuous_covariant(state_dim, action_dim).to(device)

#params = list(network.actor.parameters()) + list(network.critic.parameters())
optim = torch.optim.Adam(network.parameters(), lr = alpha, betas = (0.9,0.999), eps = 1e-8)
#optim = torch.optim.SGD(network.parameters(), lr = alpha)
#optim = torch.optim.RMSprop(network.parameters(), lr = alpha)



average = deque(maxlen = 100)
max_score = - 99999
scores = []
score = 0
av_score = 0


env_info = env.reset(train_mode=True)[brain_name] # reset the environment
state = env_info.vector_observations[0]            # get the current state


#state = env.reset()

states = deque(maxlen = rollout_length)
actions = deque(maxlen = rollout_length)
rewards = deque(maxlen = rollout_length)
dones = deque(maxlen = rollout_length)
probas = deque(maxlen = rollout_length)
returns = deque(maxlen = rollout_length)
deque_advantages = deque(maxlen = rollout_length)
values = deque(maxlen = rollout_length +1 )

episode = 1
t = 0
while True:
    t += 1
    #beta = beta * entropy_decay
    if episode == time_limit:
        break
    torch_state = torch.tensor(state, dtype=torch.float).to(device)
    
    action, proba, _ = network.act(torch_state)
    value = network.value(torch_state)
    
    #next_state, reward, done, _ = env.step(action)
    env_info = env.step(action)[brain_name]        # send the action to the environment
    next_state = env_info.vector_observations[0]   # get the next state
    reward = env_info.rewards[0]                   # get the reward
    done = env_info.local_done[0]  


    score += reward

    memory.add(
                state, 
                action, 
                reward, 
                done, 
                proba.detach().cpu().numpy(), 
                value.detach().cpu().numpy()
                )

    states.append(state)
    rewards.append(reward)
    actions.append(action)
    values.append(value)
    probas.append(proba)
    dones.append(done)

    

    if done:
        #state = env.reset()
        episode += 1

        env_info = env.reset(train_mode=True)[brain_name] # reset the environment
        next_state = env_info.vector_observations[0]            # get the current state


        average.append(score)
        scores.append(score)
        if score > max_score:
            max_score = score
        score = 0
        average.append(np.mean(average))



        if (episode % print_every == 0) & (episode>1):
            av_score = np.mean(average)
            print('\r{}/{}\t average score: {:.2f}\tmax score: {:.2f}'\
                .format(episode, time_limit, av_score, max_score ), end ='')
        if episode % keep_print_every == 0:
            av_score = np.mean(average)
            print('\r{}/{}\t average score: {:.2f}\tmax score: {:.2f}'\
                .format(episode, time_limit, av_score, max_score ))
            np.savetxt('scores.txt', scores, fmt='%f')
        if np.mean(average) > target_score :

            print("\nsolved in {} steps !".format(episode), end ='')
            torch.save(network.state_dict(), 'network_parameters.pth')
            np.savetxt('scores.txt', scores, fmt='%f')
            break





    







    # t begins at 1
    
    
    state = next_state

    if t % rollout_length == 0:
        
        # normalize rewards or advantages but not both
        normalized_rewards = rewards
        torch_state = torch.tensor(state, dtype=torch.float).to(device).detach()
        values.append(network.value(torch_state).detach())
        ret = values[- 1] #network.value(state).detach()
        advantage = 0
        for i in reversed(range(rollout_length)):
            ret = normalized_rewards[i] + discount * (1-dones[i]) * ret
            TD_error = normalized_rewards[i] + discount * (1-dones[i])*values[i+1] - values[i]
            advantage = advantage * GAE_tau * discount * (1-dones[i]) + TD_error#ret - values[i].detach()
            deque_advantages.appendleft(advantage.item())
            returns.appendleft(ret)
        # Really important to normalize
        # normalize advantage , but don't normalize rewards otherwise it introduces a bias
        # when minimize loss to learn value function
        # adjust with c_1 in interval (0,1)
        advantages = (deque_advantages - np.mean(deque_advantages)) /  (np.std(deque_advantages) + 1e-10)

        #
        for k in range(optimization_epoch):
            batches = memory.sample(advantages, returns)
        
            for batch in batches:   
                batch_states, batch_actions, _ , _ , old_probas, _, batch_returns, batch_advantages= batch
            
            
                
                new_probas, entropies = network.give_log_prob(batch_states, batch_actions)

 

                ratio = (new_probas - old_probas).exp()
                clipped_surr = torch.clamp(ratio, 1 - eps, 1 + eps) * batch_advantages
                non_clipped_surr = ratio * batch_advantages
                loss_policy = - torch.min( non_clipped_surr, clipped_surr).mean()
                              #- beta * entropies.mean()
                loss_value = c_1 * (batch_returns - network.value(batch_states)).pow(2).mean()
                loss = loss_policy + loss_value


                optim.zero_grad()
                loss.backward()
                #nn.utils.clip_grad_norm_(network.parameters(), gradient_clip)
                optim.step()

        #
        memory.clear()
    
    #



# plot result

#np.savetxt('scores.txt', scores, fmt='%f')

average = []
scores_deque = deque(maxlen=100)

for x in scores:
    scores_deque.append(x)
    average.append(np.mean(scores_deque))
goal = [target_score  for x in range(len(average))]

plt.plot(scores, 'b-', label = 'score')
plt.plot(average, 'r-', label = 'average score')
plt.plot(goal, 'k--', label = 'goal')
plt.xlabel('trajectory #')
plt.ylabel('score')
plt.legend()
plt.show()


